---
title         : "Evolution of Statistical Software and Quantitative Methods"
shorttitle    : "Evolution Software and Methods"

author: 
  - name      : "Brandon LeBeau"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Psychological and Quantitative Foundations, University of Iowa, Iowa City, IA 52245"
    email         : "brandon-lebeau@uiowa.edu"
  - name      : "Ariel M. Aloe"
    affiliation   : "1"
affiliation       :
  - id            : "1"
    institution   : "University of Iowa"

abstract: > 
  Statistical software is the enabling tool of quantitative research studies and the availability and use of the software can greatly shape which methods are used by researchers. Software that is more accessible is likely to have more users and the methods implemented within the software limits the methods accessible to researchers. Open source software, (e.g. R), has reduced these barriers by making cutting edge statistical methods available to researchers through add-on packages. This paper aims to explore the evolution of statistical software within social science research using a research synthesis to establish the state of affairs.

keywords: "Research Synthesis, Statistical Software, Quantitative Methods"
wordcount: 
  
bibliography      : ["master.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
linkcolor         : "blue"

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r rootdir, echo = FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/bleb/OneDrive - University of Iowa/JournalArticlesInProgress/software_pop")
```

# Objectives
The purpose of this paper is to explore the evolution (or lack thereof) of statistical software usage over time in education. As this usage is likely tied closely to the methods they are employed, the interaction between software usage and quantitative research methods will also be explored. Research synthesis methods [@cooper2016] will be used to explore these trends over time in published social science research journals. 

# Theoretical Framework
Statistical software is the enabling tool to performing applied data analysis. Statistical methods that are implemented within software will increase their usage (particularly if the software is also user-friendly) by applied analysts and are likely taught more frequently in methodology courses at universities. Moreover, casual users of statistics software may not distinguish between the limitations of the models and the limitations of the software. In addition, the user-friendly nature of software (i.e., point and click graphical interfaces, ability to manipulate data by hand) also can severely limit the ability for research to be reproducible; a recent topic of intense discussion in biostatistics, medicine, and pyschology [@asendorpf2013; @ioannidis2014; @iqbal2016; @peng2009; @peng2011; @stodden2012]. The reproducibility crisis has pointed the finger at statistical software more directly with a strong push in some disciplines for analyses to be script (i.e., source code) based and posted with the published journal article, often described as the gold standard.

This idea of reproducibility has not seemed to fully enter the social science research domain. SPSS is likely the most common statistical software program used in many social science research domains, particularly education. Although SPSS has many common and advanced statistical techniques and it is possible to have a reproducible analysis, the default behavior within this program is often not script based and can create bad habits (i.e., editing raw data directly in the gui interface, running analyses without saving a script, etc.). Statistical software that is primarily command line, programs such as R or Python, offer easier reproducible frameworks as all data manipulations or analyses are saved in scripts that can be re-ran in the future. A data script can be thought of as a cockpit flight recorder in which every single step that was done to the original data going from data collection to final tables and figures was script based. Under a reproducible framework, the raw data are never altered directly, they should always be altered programmatically through a script. This keeps a log of the data manipulations that happened in the data analysis cycle. For example, R has packages that aid in the ability to create living data documents that contain text and analysis code within a single document [see @rmarkdown; @knitr; @knitrmanual].

The reproducible analysis framework has many advantages, including a transparent analysis process that could be validated by others or even simply the ability to investigate the data analysis completed months or years previously. Unfortunately, the current academic research framework has many barriers that limit the reproducibility. First, applied researchers may not be users of primarily command line or script based statistical software. This limits the ability to create a reproducible framework from the start. Secondly, researchers are not incentivized to conduct an analysis in a reproducible framework. Namely, the publish or perish aspect of academic research limits the sharing of statistical code partly due to the increased chance of criticism upon evaluation of the code used for the analysis. Finally, many journals and even the American Psychological Association (APA) publication manual [@apa] states that common software or programming languages need not be cited. This could even be interpretted by some as not needing to mention. Unfortunately, if the software used for a data analysis is not reported, the ability to recreate the analysis drops even more due to differences in estimation, handling of missing data, or other software specific settings. 

This paper aims to explore the state of affairs in statistical software usage in education. Particular attention will be made to which software is currently being used in published social science research as well as how this has changed over the last twenty years. Secondly, this paper also aims to explore how frequently open-source software tools are used and to explore evidence of reproducible analysis framework being implemented. These aims will be explored using research synthesis methods.

## Research Questions
1. To what extent has the statistical software usage shifted over time in published analyses?
    + If there is evidence of a shift, is there evidence this shift differs based on quantitative method or journal?
2. To what extent are published analyses citing statistical software?
    + Has this changed over time and across journals?
3. To what extent are open-source software tools used?
    + Is there evidence of reproducible analyses being employed?

# Methods
Research synthesis methods [@cooper2016] will be used to explore the evolution of statistical software and quantitative methods in social science research. More specifically, the statistical software used for the analysis will be coded in additional to the specific quantitative methods (i.e. linear regression, hierarchical linear model, etc.). Additional meta data will also be coded including, journal, article title, author information, article keywords, year published, and any mention of supplementary materials. This information will be used to explore descriptive trends in the data over time, by journals, and methods.

The research synthesis will gather data from a handful of education journals that primarily publish empirical data analysis. The search will not include journals that the primary focus is methodological, the use of software in these journals would likely be a different population than those that are data analytic in nature. Therefore the following journals were selected to be searched from 1995 onward:

* American Economic Journal (AEJ)
* American Educational Research Journal (AERJ)
* American Journal of Political Science (AJPS)
* Economic Journal (EJ)
* Educational Evaluation and Policy Analysis (EEPA)
* Educational Researcher (ER)
* Higher Education (HE)
* Journal of Experimental Education (JEE)
* Journal of Public Policy (JPP)
* Political Science Quarterly (PSQ)
* Public Policy Administration (PPA)
* Sociology of Education (SE)

## Data and Software
All journal articles published between 1995 through the middle of 2018 will be organized into EndNote. Within EndNote, the find pdf feature will be used to gather the published documents from each journal. This pdf database will then be searched using the *pdfsearch* R package [@pdfsearch; @rpro]. This package allows for keyword searching directly within pdf documents. This will be the primary data collection method. The software keywords searched for can be seen in Table \@ref(tab:searchwords). Table \@ref(tab:searchwords) also shows keywords to be used to search for statistical models and estimation methods. A handful of articles will be randomly selected to be coded manually by reading the document to evaluate the accuracy of coding using the *pdfsearch* package. 

Additional metadata obtained from journal articles will be obtained and combined with the keyword searching data. This metadata will contain information such as, year of publication, publication keywords, author information, and other article metadata obtained from EndNote. These data will be used to further enhance the keyword search data obtained from the *pdfsearch* package and the subsequent analyses discussed in the next section.

Table: (#tab:searchwords) Search keywords used in search of published journal documents 

+---------------+-------+----------------------------+
| Search        | Group | Keywords                   |
+===============+=======+============================+
| Software      | SPSS  | SPSS Statistics, SPSS Modeler, SPSS                   |
|               | R     | R-project, R Project, CRAN, R core team, R software, RStudio |
|               | SAS   | SAS Insitute, SAS, JMP                    |
|               | STATA | STATA |
|               | Python | Python |
|               | Other | MATLAB, Statistica , Statsoft, Java, Hadoop, Minitab, Systat, Tableau, Scala, Julia, Azure Machine Learning |
|               | HLM   | HLM[0-9], HLM [0-9] |
|               | IRT   | BILOG, BILOG-MG, Multilog, PARSCALE, IRT Pro |
|               | Latent Variable |  Mplus, LISREL, AMOS|
+---------------+-------+----------------------------+
| Statistical   | ANOVA | Analysis of Variance, ANOVA, ANCOVA, Analysis of Covariance, multivariate analysis of variance, MANOVA, repeated measures analysis of variance, RMANOVA, RM-ANOVA |
| Models        | HLM | HLM, Hierarchical Linear Model, Linear Mixed Model, LMM, Multilevel Model, Multi-level Model |
|               | Latent Variable | item response theory, IRT, confirmatory factor analysis, CFA, exploratory factor analysis, EFA, latent variable modeling, structural equation modeling, SEM |
|               | t-test | one sample t-test, one-sample t-test, two sample t-test, two-sample t-test, dependent samples t-test, dependent-sample t-test | 
|               | Regression | Regression, multiple regression, linear regression, multiple linear regression, nonlinear regression, non-linear regression, logistic regression, ordinal regression, multinomial logistic regression, multinomial regression, generalized additive models, GAM, general(ized)? linear model, general(ized)? linear mixed model |
|               | Growth | growth model, latent growth model, LGM |
|               | Other | cluster analysis, hierarchical cluster analysis, propensity score matching, propensity score analysis, meta analysis, meta-analysis, nonparametric analysis, chi-square( analysis)? |
+---------------+----------------------------+

## Analysis
Descriptive analyses will be performed on the research synthesis data obtained from keyword searching performed with the *pdfsearch* package. Initial exploration will focus on software and statistical models separately. Subsequent descriptive analyses will be performed to explore the if there are any intereactions between software and statistical models used in published research. All of these analyses will be performed over time to explore trends in software and statistical model usage and citation rates. Figures will be the primary analysis methods and will be created in R using the ggplot2 package [@ggplot2].

# Results
The number and percentage of PDFs able to be obtained from EndNote is shown in Table \@ref(tab:setup). Many of the journals had a high success rate of obtaining the PDF for the published studies, however some were problematic. For example, JPP and EEPA were both under 50\% of PDFs obtained and AJPS, EJ, and SE were all just over 50\%. The PDFs that were not obtained were attempted to be obtained over multiple occasions over a six month period, with no additional PDFs obtained between the last two attempts. Figure \@ref(fig:pdf-time) shows the percentage of PDFs obtained by year and journal and highlights some noticeable trends. There are periods of time for specific journals that all the PDFs are obtained compared to not obtained. For example, AJPS, EEPA, EJ, and SE have periods from 1995 to just after 2000 where most of the PDFs were not obtained. These ranges are portions of time in which our University does not have digital access to these journals. 


```{r setup, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, results='asis'}
# number of articles gathered by journal -----
library(dplyr)

jour_art <- data.frame(
  journal = c('aej_ae', 'AERJ', 'am_j_pol_sci', 'EEPA',
              'ej', 'er', 'he', 'JEE', 'pol_sci_quar',
              'pub_policy_admin', 'public_policy',
              'SE'),
  num_pdfs = c(363, 444, 922, 188, 
               1829, 742, 1914, 517, 2722,
               83, 27, 261),
  total_articles = c(364, 444, 1436, 405,
                     3376, 794, 1914, 525, 3589,
                     83, 180, 453),
  journal2 = c('AEJ', 'AERJ', 'AJPS', 'EEPA',
               'EJ', 'ER', 'HE',
               'JEE', 'PSQ', 
               'PPA', 'JPP', 
               'SE')
) %>% 
  mutate(perc_pdf = round((num_pdfs / total_articles)*100, 1))

# explore number of articles with a match for each journal
# AERJ: 443 Articles
# EEPA: 188 Articles
# JEE: 208 Articles
library(knitr)
library(kableExtra)

jour_art %>%
  select(Journal = journal2, 'Number of PDFs' = num_pdfs, 'Total Possible Articles' = total_articles, 'Percent PDFs Obtained' = perc_pdf) %>%
  kable(booktabs = TRUE, caption = 'EndNote success rate of obtaining article PDf by journal.') %>%
  kable_styling(latex_options = c("hold_position"),
                full_width = F)
```

```{r pdf-time, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Number of PDFs obtained by journal and year."}
source("code/bibtex_processing.r")

num_year$pdf <- ifelse(num_year$pdf == 'no', 'No', 'Yes')

ggplot(num_year, aes(x = YEAR, y = I(prop_year * 100), color = pdf)) + 
  geom_line(size = 2) + 
  theme_bw() + 
  facet_wrap(~ journal2) + 
  ylab("Percentage") + 
  scale_x_continuous("Year", breaks = seq(1995, 2018, 10)) + 
  scale_color_grey("PDF?")
```

Using the obtained PDFs, keyword searching was performed for the software and model keywords. Figure \@ref(fig:count-software) shows the percentage of articles from each journal with at least one match for software and model keywords. A few trends emerge from this figure. First, software keywords are much less likely to be found within the obtained PDFs. The largest percentage was in JEE with about 50\% of obtained articles reporting at least one of the searched software keywords. Most of the other journals only had 25\% or less of the articles mentioning one of the software keywords, with none of the obtained articles in JPP mentioning a software keyword. Model keywords on the other hand were much better more prevalent and the journals fall into two broad groups. One group, JEE, EEPA, AEJ, AJPS, SE, and EJ have more than 50\% of the obtained articles mentioning at least one of the model keywords and the remaining journals being less than 50\%. The articles in the latter group, with the exception of AERJ, were closer to 25\% of less. 


```{r count-software, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Number of articles with at least one model or software keyword match by journals."}
# load in journal keyword data

# Names are: keyword_results_* where * is journal abbreviation

load('data/keyword_aej_ae_v2.rda')
load('data/keyword_aerj_v2.rda')
load('data/keyword_am_j_pol_sci_v2.rda')
load('data/keyword_eepa_v2.rda')
load('data/keyword_ej_v2.rda')
load('data/keyword_er_v2.rda')
load('data/keyword_he_v2.rda')
load('data/keyword_jee_v2.rda')
load('data/keyword_pol_sci_quar_v2.rda')
load('data/keyword_pub_policy_admin_v2.rda')
load('data/keyword_public_policy_v2.rda')
load('data/keyword_SE_v2.rda')
# combine
library(dplyr)

software_keywords <- bind_rows(
  keyword_results_aej_ae,
  keyword_results_aerj, 
  keyword_results_am_j_pol_sci,
  keyword_results_eepa, 
  keyword_results_ej,
  keyword_results_er,
  keyword_results_he,
  keyword_results_jee,
  keyword_results_pol_sci_quar,
  keyword_results_pub_policy_admin,
  keyword_results_public_policy,
  keyword_results_SE
) %>%
  mutate(group = 'software')

# Model Keywords ----
load('data/keyword_aej_ae_model_v2.rda')
load('data/keyword_aerj_model_v2.rda')
load('data/keyword_am_j_pol_sci_model_v2.rda')
load('data/keyword_eepa_model_v2.rda')
load('data/keyword_ej_model_v2.rda')
load('data/keyword_er_model_v2.rda')
load('data/keyword_he_model_v2.rda')
load('data/keyword_jee_model_v2.rda')
load('data/keyword_pol_sci_quar_model_v2.rda')
load('data/keyword_pub_policy_admin_model_v2.rda')
load('data/keyword_public_policy_model_v2.rda')
load('data/keyword_SE_model_v2.rda')

# combine
model_keywords <- bind_rows(
  keyword_results_aej_ae,
  keyword_results_aerj, 
  keyword_results_am_j_pol_sci,
  keyword_results_eepa, 
  keyword_results_ej,
  keyword_results_er,
  keyword_results_he,
  keyword_results_jee,
  keyword_results_pol_sci_quar,
  keyword_results_pub_policy_admin,
  keyword_results_public_policy,
  keyword_results_SE
) %>%
  mutate(group = 'model')

# bind rows together ----
keywords <- bind_rows(software_keywords, model_keywords) %>%
  select(journal, everything()) %>%
  arrange(journal, pdf_name)

# extract year from pdf_name
mat <- regexpr("-[0-9]{4}-", keywords$pdf_name)

keywords <- keywords %>%
  mutate(year2 = regmatches(keywords$pdf_name, mat), 
         year = gsub("-", "", year2))

keywords <- keywords %>%
  left_join(jour_art, by =)

# Remove ' R ' keyword ----
keywords_nor <- keywords %>%
  filter(keyword != ' R ', keyword != '[:alpha:] package',
         keyword != 'Julia', keyword != 'eta-analysis')
  
# Descriptive statistics ----
# number of artcles with software/model mentioned
num_articles <- keywords_nor %>%
  group_by(group, journal) %>%
  summarise(num_ids = length(unique(ID))) %>%
  left_join(jour_art, by = 'journal') %>%
  mutate(percent_keyword = num_ids / num_pdfs,
         group2 = ifelse(group == 'model', 'Model', 'Software'))

library(ggplot2)
library(forcats)

ggplot(num_articles, aes(x = fct_reorder(journal2, percent_keyword), 
                         y = I(percent_keyword * 100))) + 
  geom_bar(stat = 'identity') + 
  theme_bw() + 
  coord_flip() + 
  xlab("Journals") + 
  ylab("Percentage") + 
  facet_wrap(~ group2)
```

```{r software-journal, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Software counts by journal", fig.height = 10}

library(forcats)
keywords_nor <- keywords_nor %>%
  mutate(keyword2 = fct_recode(keyword,
            "R" = "R-project" ,
            "R" = "R project",
            "R" = "CRAN",
            "R" = "R core team",
            "R" = "R software",
            "R" = "RStudio",
            "SAS" = "SAS Institute",
            "SAS" = "JMP",
            'SPSS' = "SPSS Statistics",
            'HLM' = 'HLM[0-9]',
            'HLM' = 'HLM [0-9]',
            'IRT' = 'BILOG',
            'IRT' = 'BILOG-MG',
            'IRT' = 'IRT PRO',
            'Other' = 'MATLAB',
            'Mplus' = 'M-Plus',
            'IRT' = 'Multilog',
            'IRT' = 'PARSCALE',
            'Other' = 'Scala',
            'Other' = 'Statistica ',
            'Other' = 'Systat',
            # Model recode
            'ANOVA' = 'Analysis of Covariance',
            'ANOVA' = 'Analysis of Variance',
            'ANOVA' = 'ANCOVA',
            'ANOVA' = 'repeated measures analysis of variance',
            'ANOVA' = 'MANOVA',
            'ANOVA' = 'RM-ANOVA',
            'ANOVA' = 'multivariate analysis of variance',
            'CFA' = 'confirmatory factor analysis',
            'EFA' = 'exploratory factor analysis',
            'GAM' = 'generalized additive models',
            'Cluster Analysis' = 'hierarchical cluster analysis',
            'Chi-Square' = 'chi-square( analysis)?',
            'Chi-Square' = 'nonparametric analysis',
            't-test' = 'dependent samples t-test',
            't-test' = 'one-sample t-test',
            't-test' = 'two-sample t-test',
            't-test' = 'two sample t-test',
            'SEM' = 'structural equation modeling',
            'SEM' = 'latent variable modeling',
            'Meta-analysis' = 'meta analysis',
            'Growth' = 'growth model',
            'Growth' = 'latent growth model',
            'Growth' = 'LGM',
            'Linear Mixed Model' = 'HLM',
            'Linear Mixed Model' = 'Hierarchical Linear Model',
            'Linear Mixed Model' = 'LMM',
            'Linear Mixed Model' = 'Multi-level Model',
            'Linear Mixed Model' = 'Multilevel Model',
            'Linear Mixed Model' = 'general(ized)? linear mixed model',
            'Linear Model' = 'general(ized)? linear model',
            'Linear Model' = 'linear regression',
            'Linear Model' = 'Regression',
            'Linear Model' = 'multiple regression',
            'Linear Model' = 'multiple linear regression',
            'IRT' = 'item response theory',
            'Propensity Score' = 'propensity score analysis',
            'Propensity Score' = 'propensity score matching',
            'Logistic Regression' = 'multinomial logistic regression',
            'Logistic Regression' = 'multinomial regression',
            'Logistic Regression' = 'ordinal regression',
            'Non-linear Regression' = 'nonlinear regression'
                               ))

# plot unique keyword counts by journal
count_keyword <- keywords_nor %>%
  group_by(group, journal2, keyword2) %>%
  summarise(num_ids = length(unique(ID))) %>%
  left_join(jour_art, by = 'journal2') %>%
  mutate(percent_keyword = num_ids / num_pdfs)

ggplot(dplyr::filter(count_keyword, group == 'software', (keyword2 != 'Tableau' | keyword2 != 'Minitab')), 
       aes(x = fct_reorder(keyword2, percent_keyword), 
           y = I(percent_keyword*100))) + 
  geom_bar(stat = 'identity') +
  theme_bw() + 
  xlab("Keyword") + 
  ylab("Percentage") + 
  coord_flip() + 
  facet_wrap(~ journal2)


num_keywords <- keywords_nor %>%
  select(group, journal2, ID, keyword2) %>%
  distinct() %>%
  group_by(group, journal2, ID) %>%
  summarise(num = n()) %>%
  summarise(avg_num = mean(num),
            min_num = min(num),
            max_num = max(num))

```


Expanding on the software keywords found within the PDFs, Figure \@ref(fig:software-journal) explores which software keywords were found in each journal. In general, mirroring results from Figure \@ref(fig:count-software), the percetage of articles reporting software keywords was relatively small, most often less than 5\%. R, SAS, SPSS, and STATA were the most commonly found software keywords, with R being the most common in most journals. The one exception to this was in JEE, where SAS was more common (about 20\% of articles) and R and SPSS had a similar percentage (about 15\%) of articles reporting their usage. One interpretation note, articles may mention more than one software keyword and those duplicate results will show up in each category. On average, the average number of software keywords identified in each article was highest for JEE at 1.71 (range: 1 to 5) and a lowest of 1 for PPA (range: 1 to 1).

A similar figure for model keywords can be seen in Figure \@ref(fig:model-journal). The x-axis scale here is wider compared to the software keywords showing that the methods are more commonly reported. However, there are still a sizeable number of articles appearing in these journals that do not list one of the model keywords searched. The most commonly used methods are linear model, analysis of variance, meta-analysis, or linear mixed model (i.e. HLM) models. The journals EEPA, JEE, and SE have the widest array of models being picked up through the keyword search. On the opposite side, AEJ, AJPS, and EJ are dominated by linear models. Finally, journals such as AERJ, ER, HE, PPA, and PSQ all have a low prevalence of articles that are using the model keywords.

```{r model-journal, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Statistical Model counts by journal", fig.height = 10}
ggplot(dplyr::filter(count_keyword, group == 'model'), 
       aes(x = fct_reorder(keyword2, percent_keyword), 
           y = I(percent_keyword*100))) + 
  geom_bar(stat = 'identity') +
  theme_bw() + 
  xlab("Keyword") + 
  ylab("Percentage") + 
  coord_flip() + 
  facet_wrap(~ journal2)
```



## Interaction between software and statistical methods





# Discussion


# References
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}



```{r software-year-at1, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Software percentages by year", fig.height = 10}
num_articles <- keywords_nor %>%
  group_by(group, year, journal, keyword2) %>%
  summarise(num_ids = length(unique(ID))) %>%
  ungroup() %>%
  mutate(year = as.numeric(year)) %>%
  left_join(filter(num_year, pdf == 'Yes'), by = c('journal', 'year' = 'YEAR')) %>%
  mutate(prop_keyword = num_ids / num,
         perc_keyword = round(prop_keyword * 100, 1)) %>%
  select(group, year, keyword2, perc_keyword) %>%
  group_by(group, year, keyword2) %>%
  summarise(perc_keyword = mean(perc_keyword))

software_keywords <- filter(num_articles, group == 'software', 
                            keyword2 %in% c('R', 'SAS', 'SPSS', 'STATA')) %>%
  mutate(prop_keyword = perc_keyword / 100)

library(patchwork)
# The years need to be represented as columns.
# Need to also find the total number of articles by year for each journal.
prim <- ggplot(software_keywords, aes(x = year, y = perc_keyword, color = keyword2)) + 
    geom_line(aes(group = keyword2), size = 1.25, alpha = 0.6) + 
    theme_bw() + 
    #geom_text(aes(x = year, y = perc_keyword, label = round(perc_keyword, 0)), size = 5) + 
    scale_y_continuous('Percentage', breaks = seq(0, 30, 3), limits = c(0, 28), expand = c(0,0)) +
    scale_x_continuous("Year", breaks = c(seq(1995, 2018, 5), 2018)) +
    scale_color_grey('', start = 0, end = 0.7) + 
    # geom_text(data = filter(software_keywords, year == 1995), 
    #           aes(x = year - 1, y = perc_keyword, label = keyword2))  + 
    # geom_text(data = filter(software_keywords, year == 2018), 
    #           aes(x = year + 1, y = perc_keyword, label = keyword2)) + 
  NULL #ggtitle('Software citation percentages across all journals over time')

software_keywords <- filter(num_articles, group == 'software', 
                            keyword2 %in% c('AMOS', 'HLM', 'LISREL', 'Mplus')) %>%
  mutate(prop_keyword = perc_keyword / 100)

# The years need to be represented as columns.
# Need to also find the total number of articles by year for each journal.
spec <- ggplot(software_keywords, aes(x = year, y = perc_keyword)) + 
    geom_line(aes(group = keyword2, color = keyword2), size = 1.25, alpha = 0.6) + 
    theme_bw() + 
    #geom_text(aes(x = year, y = perc_keyword, label = round(perc_keyword, 0)), size = 5) + 
    scale_y_continuous('Percentage', breaks = seq(0, 20, 2), limits = c(0, 20), expand = c(0,0)) +
    scale_x_continuous("Year", breaks = c(seq(1995, 2018, 5), 2018)) +
    scale_color_grey('', start = 0, end = 0.7) + 
    # geom_text(data = filter(software_keywords, year == 1995), 
    #           aes(x = year - 1, y = perc_keyword, label = keyword2))  + 
    # geom_text(data = filter(software_keywords, year == 2018), 
    #           aes(x = year + 1, y = perc_keyword, label = keyword2)) + 
  NULL #ggtitle('Specialty software citation percentages across all journals over time')

prim / spec
```


```{r model-year-at1, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Model percentages by year", fig.height = 10}
model_keywords <- filter(num_articles, group == 'model', 
                         keyword2 %in% c('ANOVA', 'Linear Model', 'Linear Mixed Model', 'Logistic Regression', 't-test')) #%>%
  # mutate(keyword3 = ifelse(keyword2 == 'Linear Model', 'LM', ifelse(keyword2 == 'Linear Mixed Model', "LMM", ifelse(keyword2 == 'Logistic Regression', 'LR', keyword2)))) %>%
  # mutate(prop_keyword = perc_keyword / 100)

# The years need to be represented as columns.
# Need to also find the total number of articles by year for each journal.
mod1 <- ggplot(model_keywords, aes(x = year, y = perc_keyword)) + 
    geom_line(aes(group = keyword2, color = keyword2), size = 1.25, alpha = 0.6) + 
    theme_bw() + 
    #geom_text(aes(x = year, y = perc_keyword, label = round(perc_keyword, 0)), size = 5) + 
    scale_y_continuous('Percentage', breaks = seq(0, 60, 5), limits = c(0, 60), expand = c(0,0)) +
    scale_x_continuous("Year", c(seq(1995, 2018, 5), 2018)) +
    scale_color_grey('', start = 0, end = 0.7) + 
    # geom_text(data = filter(model_keywords, year == 1995), 
    #           aes(x = year - 2, y = perc_keyword, label = keyword3))  + 
    # geom_text(data = filter(model_keywords, year == 2018), 
    #           aes(x = year + 2, y = perc_keyword, label = keyword3)) + 
  NULL #ggtitle('Model citation percentages across all journals over time')

model_keywords <- filter(num_articles, group == 'model', 
                         keyword2 %in% c('IRT', 'CFA', 'EFA', 'SEM'))  %>%
  mutate(prop_keyword = perc_keyword / 100)

# The years need to be represented as columns.
# Need to also find the total number of articles by year for each journal.
mod2 <- ggplot(model_keywords, aes(x = year, y = perc_keyword)) + 
    geom_line(aes(group = keyword2, color = keyword2), size = 1.25, alpha = 0.6) + 
    theme_bw() + 
    #geom_text(aes(x = year, y = perc_keyword, label = round(perc_keyword, 0)), size = 5) + 
    scale_y_continuous('Percentage', breaks = seq(0, 20, 2), limits = c(0, 18), expand = c(0,0)) +
    scale_x_continuous("Year",  breaks = c(seq(1995, 2018, 5), 2018)) +
    scale_color_grey('', start = 0, end = 0.7) + 
    # geom_text(data = filter(model_keywords, year == 1995), 
    #           aes(x = year - 1, y = perc_keyword, label = keyword2))  + 
    # geom_text(data = filter(model_keywords, year == 2018), 
    #           aes(x = year + 1, y = perc_keyword, label = keyword2)) + 
  NULL #ggtitle('Latent variable model citation percentages across all journals over time')

mod1 / mod2
```


```{r area-plot-model, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Area plot", eval = FALSE}
ggplot(model_keywords, aes(x = year, y = perc_keyword, fill = keyword2)) + 
  geom_area(position = 'fill', color = 'black') + 
  viridis::scale_fill_viridis(discrete = TRUE)
```


```{r software-year, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Software counts by year"}
count_keyword <- keywords_nor %>%
  group_by(group, journal2, year, keyword2) %>%
  summarise(num_ids = length(unique(ID))) %>%
  left_join(jour_art, by = 'journal2') %>%
  mutate(percent_keyword = num_ids / num_pdfs,
         year2 = as.integer(year)) %>%
  group_by(group, year2, keyword2) %>%
  summarise(percent_keyword = mean(percent_keyword))

count_model <- dplyr::filter(count_keyword, group == 'software')

```


```{r model-year, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Software counts by year"}
count_model <- dplyr::filter(count_keyword, group == 'model')

```

```{r software_statmethods, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.cap = "Tile plot showing interaction between software and statistical methods.", eval = FALSE}
# spread data to wide for tile plot
keyword_fj <- full_join(software_keywords,
                        model_keywords,
                        by = c('pdf_name', 'journal')) %>%
  left_join(jour_art) %>%
  filter(keyword.x != ' R ', keyword.x != '[:alpha:] package',
         keyword.x != 'Julia', keyword.x != 'eta-analysis') %>%
  mutate(keyword2.x = fct_recode(keyword.x,
                               "R" = "R-project" ,
                               "R" = "R project",
                               "R" = "CRAN",
                               "R" = "R core team",
                               "R" = "R software",
                               "R" = "RStudio",
                               "SAS" = "SAS Institute",
                               "SAS" = "JMP",
                               'SPSS' = "SPSS Statistics",
                               'HLM' = 'HLM[0-9]',
                               'HLM' = 'HLM [0-9]',
                               'IRT' = 'BILOG',
                               'IRT' = 'BILOG-MG',
                               'IRT' = 'IRT PRO',
                               'Other' = 'MATLAB',
                               'Mplus' = 'M-Plus',
                               'IRT' = 'Multilog',
                               'IRT' = 'PARSCALE',
                               'Other' = 'Scala',
                               'Other' = 'Statistica ',
                               'Other' = 'Systat'),
         keyword2.y = fct_recode(keyword.y,
                               # Model recode
                               'ANOVA' = 'Analysis of Covariance',
                               'ANOVA' = 'Analysis of Variance',
                               'ANOVA' = 'ANCOVA',
                               'ANOVA' = 'repeated measures analysis of variance',
                               'ANOVA' = 'MANOVA',
                               'ANOVA' = 'RM-ANOVA',
                               'ANOVA' = 'multivariate analysis of variance',
                               'CFA' = 'confirmatory factor analysis',
                               'EFA' = 'exploratory factor analysis',
                               'GAM' = 'generalized additive models',
                               'Cluster Analysis' = 'hierarchical cluster analysis',
                               'Chi-Square' = 'chi-square( analysis)?',
                               'Chi-Square' = 'nonparametric analysis',
                               't-test' = 'dependent samples t-test',
                               't-test' = 'one-sample t-test',
                               't-test' = 'two-sample t-test',
                               't-test' = 'two sample t-test',
                               'SEM' = 'structural equation modeling',
                               'SEM' = 'latent variable modeling',
                               'Meta-analysis' = 'meta analysis',
                               'Growth' = 'growth model',
                               'Growth' = 'latent growth model',
                               'Growth' = 'LGM',
                               'Linear Mixed Model' = 'HLM',
                               'Linear Mixed Model' = 'Hierarchical Linear Model',
                               'Linear Mixed Model' = 'LMM',
                               'Linear Mixed Model' = 'Multi-level Model',
                               'Linear Mixed Model' = 'Multilevel Model',
                               'Linear Mixed Model' = 'general(ized)? linear mixed model',
                               'Linear Model' = 'general(ized)? linear model',
                               'Linear Model' = 'linear regression',
                               'Linear Model' = 'Regression',
                               'Linear Model' = 'multiple regression',
                               'Linear Model' = 'multiple linear regression',
                               'IRT' = 'item response theory',
                               'Propensity Score' = 'propensity score analysis',
                               'Propensity Score' = 'propensity score matching',
                               'Logistic Regression' = 'multinomial logistic regression',
                               'Logistic Regression' = 'multinomial regression',
                               'Logistic Regression' = 'ordinal regression',
                               'Non-linear Regression' = 'nonlinear regression'
  ))

count_keyword <- keyword_fj %>%
  group_by(journal2, keyword2.x, keyword2.y) %>%
  summarise(num_ids = length(unique(pdf_name))) %>%
  left_join(jour_art, by = 'journal2') %>%
  mutate(prop_keyword = num_ids / num_pdf,
         percent_keyword = prop_keyword * 100)

# library(gganimate)
library(viridis)
ggplot(count_keyword, aes(x = keyword2.x, y = keyword2.y)) + 
  geom_raster(aes(fill = percent_keyword)) + 
  xlab("Software") +
  ylab("Models") +
  theme_bw() + 
  scale_fill_viridis('Percent')
```
